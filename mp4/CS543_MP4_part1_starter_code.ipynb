{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS543_MP4_part1_starter_code.ipynb","provenance":[{"file_id":"1byEEekMrA5kdUP6YPL5cEo3qP5Q-Sxxx","timestamp":1587443002620},{"file_id":"1z99mRbVNHe2UoXdzNLNqbgW8VFDlNPmN","timestamp":1586649144726},{"file_id":"1GNj505yBVMqRHa1P3Fi_PBED7_MVmmRe","timestamp":1586111255621},{"file_id":"15H_OJEOR4-KU8-UJ7Ieo2WdTbMFV5RaJ","timestamp":1585604402819}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b71add10229c4282a57002ae17efe97b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ab31654277c243bcb8dc1bbf1587d6a3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b44155713f0c4b4c932e7851a5e92b86","IPY_MODEL_e1a051c2d8bf4e0b83bc52722b1a261e"]}},"ab31654277c243bcb8dc1bbf1587d6a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b44155713f0c4b4c932e7851a5e92b86":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_72ce018a7f8f405eb50667a7bb728f93","_dom_classes":[],"description":"100%","_model_name":"IntProgressModel","bar_style":"success","max":15,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":15,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b9bcca17d99f4964aff257ff29c7f478"}},"e1a051c2d8bf4e0b83bc52722b1a261e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1b207a6636af4103b0a7de4b5298371e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 15/15 [03:38&lt;00:00, 14.57s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f755eaac934e48d98f8e255706fbbfd0"}},"72ce018a7f8f405eb50667a7bb728f93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b9bcca17d99f4964aff257ff29c7f478":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b207a6636af4103b0a7de4b5298371e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f755eaac934e48d98f8e255706fbbfd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wmCbxoDrADcF"},"source":["# Google Colab setup with Google Drive folder\n","\n","This notebook provides the code you need to set up Google Colab to run and import files from within a Google Drive folder.\n","\n","This will allow you to upload assignment code to your Google Drive and then run the code on Google Colab machines (with free GPUs if needed). \n","\n","You will need to create a folder in your Google Drive to hold your assignments and you will need to open Colaboratory within this folder before running the set up code (check the link above to see how).\n","\n","Note: this use of Google Drive is optional, and you could also just manually copy the data into your colab runtime. Keep in mind, this won't be persistent though, and you will have to download your models / plots before the runtime shuts down."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zWhrmhqVCyGH"},"source":["# Mount Google Drive\n","\n","This will allow the Colab machine to access Google Drive folders by mounting the drive on the machine. You will be asked to copy and paste an authentication code."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Wv2oKmF9AJtI","outputId":"4a77d645-88f9-46ee-e136-d2daf6c36dc4","executionInfo":{"status":"ok","timestamp":1588112403175,"user_tz":300,"elapsed":3409,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kKGxaMcmP_Et","colab_type":"code","outputId":"4191ce7f-87d8-406c-98f7-e29d6225bbfb","executionInfo":{"status":"ok","timestamp":1588112406322,"user_tz":300,"elapsed":6543,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["ls"],"execution_count":18,"outputs":[{"output_type":"stream","text":["cs543-simple-val.pdf  Improve3.png  improve5.png  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n","\u001b[01;34mdata\u001b[0m/                 improve4.png  plot.png      pytorch_unet.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Qs04PPwDOFy"},"source":["# Change directory to allow imports\n","\n","\n","As noted above, you should create a Google Drive folder to hold all your assignment files. You will need to add this code to the top of any python notebook you run to be able to import python files from your drive assignment folder (you should change the file path below to be your own assignment folder)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UA2-UyfpEc9O","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/CS_543_MP4\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyRCWAIyRHWc","colab_type":"code","outputId":"edd07244-b5a0-4a83-d993-0cd8c937c12a","executionInfo":{"status":"ok","timestamp":1588112408518,"user_tz":300,"elapsed":8657,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["ls # Check if this is your MP4 folder"],"execution_count":20,"outputs":[{"output_type":"stream","text":["cs543-simple-val.pdf  Improve3.png  improve5.png  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n","\u001b[01;34mdata\u001b[0m/                 improve4.png  plot.png      pytorch_unet.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fCEPi8edenbY","colab_type":"text"},"source":["# Download data into your data folders\n","\n","Within the folder CS_543_MP4, create a subfolder called 'data'. Copy all the files in this Google drive link(https://drive.google.com/drive/folders/18wgGNdbkptqnb8W8Ao_0rQ0o-GQdHils?usp=sharing) to your data folder. Please do not change the file structures."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DDU5aVgR9QBx"},"source":["# Set up GPU and PyTorch\n","\n","First, ensure that your notebook on Colaboratory is set up to use GPU. After opening the notebook on Colaboratory, go to Edit>Notebook settings, select Python 3 under \"Runtime type,\" select GPU under \"Hardware accelerator,\" and save.\n","\n","Next, install PyTorch:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kjbQtzKT9Uc2","outputId":"382ff40d-8def-4ce1-9644-c28e3ac26be9","executionInfo":{"status":"ok","timestamp":1588112411892,"user_tz":300,"elapsed":12019,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!pip3 install torch torchvision"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u_BekZYY9Vzx"},"source":["Make sure that pytorch is installed and works with GPU:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8TXSJWQa9efx","outputId":"f3237899-1e7b-4a10-9657-6a7da5fef490","executionInfo":{"status":"ok","timestamp":1588112411893,"user_tz":300,"elapsed":12010,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch\n","a = torch.Tensor([1]).cuda()\n","print(a)\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["tensor([1.], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OEeRNsCjRXZK","colab_type":"code","outputId":"dcac290b-4c24-4c67-832a-3c84f2caabf6","executionInfo":{"status":"ok","timestamp":1588112411893,"user_tz":300,"elapsed":11999,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["torch.cuda.is_available()"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"qChgLJERsvZP","colab_type":"text"},"source":["# Part 1"]},{"cell_type":"code","metadata":{"id":"IlyCnvf6WzjR","colab_type":"code","outputId":"b22b28c4-0361-4c06-e3c7-5070bf076add","executionInfo":{"status":"ok","timestamp":1588112411894,"user_tz":300,"elapsed":11990,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\"Headers\"\"\"\n","\n","from __future__ import print_function\n","from PIL import Image\n","import os\n","import os.path\n","import numpy as np\n","import sys\n","if sys.version_info[0] == 2:\n","    import cPickle as pickle\n","else:\n","    import pickle\n","\n","import torch.utils.data as data\n","from torchvision.datasets.utils import download_url, check_integrity\n","import copy\n","import csv\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os.path\n","from tqdm.notebook import tqdm\n","import sys\n","import torch\n","import torch.utils.data\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","np.random.seed(111)\n","torch.cuda.manual_seed_all(111)\n","torch.manual_seed(111)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fe789f2ddd0>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"JpFMv7HtcII4","colab_type":"text"},"source":["\n","Training an image classifier\n","----------------------------\n","\n","We will do the following steps in order:\n","\n","1. Load the randomized FashionMNIST training, validation and test datasets using\n","   torchvision. Use torchvision.transforms to apply transforms on the\n","   dataset.\n","2. Define a Convolution Neural Network - BaseNet\n","3. Define a loss function and optimizer\n","4. Train the network on training data and check performance on val set.\n","   Plot train loss and validation accuracies.\n","5. Try the network on test data and create .csv file for submission to kaggle"]},{"cell_type":"code","metadata":{"id":"Ld6juH34dWWq","colab_type":"code","colab":{}},"source":["# <<TODO#5>> Based on the val set performance, decide how many\n","# epochs are apt for your model.\n","# ---------\n","EPOCHS = 15\n","# ---------\n","\n","IS_GPU = True\n","TEST_BS = 256\n","TOTAL_CLASSES = 10\n","TRAIN_BS = 32\n","PATH_TO_FASHIONMNIST_CS543 = \"data/fashionMNIST/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d57CSAj1dfix","colab_type":"code","colab":{}},"source":["def calculate_accuracy(dataloader, is_gpu):\n","    \"\"\" Util function to calculate val set accuracy,\n","    both overall and per class accuracy\n","    Args:\n","        dataloader (torch.utils.data.DataLoader): val set \n","        is_gpu (bool): whether to run on GPU\n","    Returns:\n","        tuple: (overall accuracy, class level accuracy)\n","    \"\"\"    \n","    correct = 0.\n","    total = 0.\n","    predictions = []\n","\n","    class_correct = list(0. for i in range(TOTAL_CLASSES))\n","    class_total = list(0. for i in range(TOTAL_CLASSES))\n","\n","    for data in dataloader:\n","        images, labels = data\n","        if is_gpu:\n","            images = images.cuda()\n","            labels = labels.cuda()\n","        outputs = net(Variable(images))\n","        _, predicted = torch.max(outputs.data, 1)\n","        predictions.extend(list(predicted.cpu().numpy()))\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum()\n","\n","        c = (predicted == labels).squeeze()\n","        for i in range(len(labels)):\n","            label = labels[i]\n","            class_correct[label] += c[i]\n","            class_total[label] += 1\n","\n","    class_accuracy = 100 * np.divide(class_correct, class_total)\n","    return 100*correct/total, class_accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aq2qOUaJeAWJ","colab_type":"text"},"source":["1.**Loading FashionMNIST_CS543**\n","\n","We modify the dataset to create FashionMNIST_CS543 dataset which consist of 60000 training images, 5000 validation images and 5000 test images. The train and val datasets have labels while all the labels in the test set are set to 0.\n"]},{"cell_type":"code","metadata":{"id":"C2UcDZmtdfq3","colab_type":"code","outputId":"23c8bd6d-b972-4727-d3f8-7e123c8a904d","executionInfo":{"status":"ok","timestamp":1588112411895,"user_tz":300,"elapsed":11962,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# The output of torchvision datasets are PILImage images of range [0, 1].\n","# Using transforms.ToTensor(), transform them to Tensors of normalized range\n","# [-1, 1].\n","\n","\n","# <<TODO#1>> Use transforms.Normalize() with the right parameters to \n","# make the data well conditioned (zero mean, std dev=1) for improved training.\n","# <<TODO#2>> Try using transforms.RandomCrop() and/or transforms.RandomHorizontalFlip()\n","# to augment training data.\n","# After your edits, make sure that test_transform should have the same data\n","# normalization parameters as train_transform\n","# You shouldn't have any data augmentation in test_transform (val or test data is never augmented).\n","# ---------------------\n","# notice for some transform like transforms.RandomRotation, given we are dealing with grayscale input, you need to put fill=(0,) in the parameter. \n","train_transform = transforms.Compose([\n","                  # transforms.RandomHorizontalFlip(),\n","                  transforms.ToTensor() \n","                  # transforms.Normalize(mean = (0.5, ), std = (0.5, ))\n","                  ])\n","\n","test_transform = transforms.Compose(\n","    [transforms.ToTensor() \n","    #  transforms.Normalize(mean = (0.5, ), std = (0.5, ))\n","     ])\n","# ---------------------\n","\n","#DO NOT CHANGE any line below\n","train_dataset = torchvision.datasets.FashionMNIST(root = \"data\", train=True, transform=train_transform, target_transform=None, download=False) \n","test_dataset = torchvision.datasets.FashionMNIST(root = \"data\", train=False, transform=test_transform, target_transform=None, download=False)\n","val_dataset = copy.deepcopy(test_dataset)\n","val_dataset.data = torch.load(\"data/FashionMNIST/processed/val.pt\")[0]\n","val_dataset.targets = torch.load(\"data/FashionMNIST/processed/val.pt\")[1]\n","print(\"train_dataset data shape: \", train_dataset.data.shape)\n","print(\"train_dataset labels shape: \", train_dataset.targets.shape)\n","print()\n","print(\"val_dataset data shape: \", val_dataset.data.shape)\n","print(\"val_dataset labels shape\",val_dataset.targets.shape)\n","\n","# check for Dataloader function: https://pytorch.org/docs/stable/data.html\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BS, shuffle=True, num_workers=2, drop_last=True)  #DO NOT CHANGE\n","valloader = torch.utils.data.DataLoader(val_dataset, batch_size=TEST_BS, shuffle=False, num_workers=2, drop_last=False) #DO NOT CHANGE\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_BS, shuffle=False, num_workers=2, drop_last=False) #DO NOT CHANGE\n","\n","# The 10 classes for FashionMNIST\n","classes = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"],"execution_count":27,"outputs":[{"output_type":"stream","text":["train_dataset data shape:  torch.Size([60000, 28, 28])\n","train_dataset labels shape:  torch.Size([60000])\n","\n","val_dataset data shape:  torch.Size([5000, 28, 28])\n","val_dataset labels shape torch.Size([5000])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5b_fBznndp4W","colab_type":"code","colab":{}},"source":["########################################################################\n","# 2. Define a Convolution Neural Network\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","# We provide a basic network that you should understand, run and\n","# eventually improve\n","# <<TODO>> Add more conv layers\n","# <<TODO>> Add more fully connected (fc) layers\n","# <<TODO>> Add regularization layers like Batchnorm.\n","#          nn.BatchNorm2d after conv layers:\n","#          http://pytorch.org/docs/master/nn.html#batchnorm2d\n","#          nn.BatchNorm1d after fc layers:\n","#          http://pytorch.org/docs/master/nn.html#batchnorm1d\n","# This is a good resource for developing a CNN for classification:\n","# http://cs231n.github.io/convolutional-networks/#layers\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BaseNet(nn.Module):\n","    def __init__(self):\n","        super(BaseNet, self).__init__()\n","        \n","        # <<TODO#3>> Add more conv layers with increasing \n","        # output channels\n","        # <<TODO#4>> Add normalization layers after conv\n","        # layers (nn.BatchNorm2d)\n","\n","        # Also experiment with kernel size in conv2d layers (say 3\n","        # inspired from VGGNet)\n","        # To keep it simple, keep the same kernel size\n","        # (right now set to 5) in all conv layers.\n","        # Do not have a maxpool layer after every conv layer in your\n","        # deeper network as it leads to too much loss of information.\n","\n","        self.conv1 = nn.Conv2d(1, 6, 3)\n","        self.bn1 = nn.BatchNorm2d(6)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 3)\n","        self.bn2 = nn.BatchNorm2d(16)\n","        self.conv3 = nn.Conv2d(16,64,3)\n","        self.bn3 = nn.BatchNorm2d(64)\n","        self.conv4 = nn.Conv2d(64,128,3)\n","        self.bn4 = nn.BatchNorm2d(128)\n","        self.conv5 = nn.Conv2d(128,256,3)\n","        self.bn5 = nn.BatchNorm2d(256)\n","\n","\n","        # <<TODO#3>> Add more linear (fc) layers\n","        # <<TODO#4>> Add normalization layers after linear and\n","        # experiment inserting them before or after ReLU (nn.BatchNorm1d)\n","        # More on nn.sequential:\n","        # http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\n","        \n","        # self.fc_net = nn.Sequential(\n","        #     nn.Linear(128 * 1 * 1, TOTAL_CLASSES//2),\n","        #     # nn.Linear(16 * 4 * 4, TOTAL_CLASSES//2),\n","        #     nn.ReLU(inplace=True),\n","        #     nn.Linear(TOTAL_CLASSES//2, TOTAL_CLASSES),\n","        # )\n","\n","        self.fc_net = nn.Sequential(\n","            nn.Linear(256 * 1 * 1, 128),\n","            nn.BatchNorm1d(128),\n","            # nn.Linear(16 * 4 * 4, TOTAL_CLASSES//2),\n","            nn.ReLU(inplace=True),\n","            # nn.BatchNorm1d(128),\n","            nn.Linear(128, TOTAL_CLASSES//2),\n","            nn.BatchNorm1d(TOTAL_CLASSES//2),\n","            nn.ReLU(inplace=True),\n","            # nn.BatchNorm1d(TOTAL_CLASSES//2),\n","            nn.Linear(TOTAL_CLASSES//2, TOTAL_CLASSES),\n","        )\n","\n","    def forward(self, x):\n","\n","        # <<TODO#3&#4>> Based on the above edits, you'll have\n","        # to edit the forward pass description here.\n","\n","        # x = self.pool(F.relu(self.conv1(x)))\n","        # Output size = 24//2 x 24//2 = 12 x 12\n","        x = self.pool(F.relu(self.bn2(self.conv2(F.relu(self.bn1(self.conv1(x)))))))\n","        # Output size = 24//2 x 24//2 = 12 x 12\n","\n","        # x = self.pool(F.relu(self.conv2(x)))\n","        # Output size = 8//2 x 8//2 = 4 x 4\n","        x = self.pool(F.relu(self.bn4(self.conv4(F.relu(self.bn3(self.conv3(x)))))))\n","        # Output size = 8//2 x 8//2 = 4 x 4\n","\n","        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n","        # Output size = 2//2 x 2//2 = 1 x 1\n","\n","        # See the CS231 link to understand why this is 16*5*5!\n","        # This will help you design your own deeper network\n","        # x = x.view(-1, 16 * 4 * 4)\n","        x = x.view(-1,256 * 1 * 1)\n","        x = self.fc_net(x)\n","\n","        # No softmax is needed as the loss function in step 3\n","        # takes care of that\n","        \n","        return x\n","\n","# Create an instance of the nn.module class defined above:\n","net = BaseNet()\n","\n","# For training on GPU, we need to transfer net and data onto the GPU\n","# http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu\n","if IS_GPU:\n","    net = net.cuda()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KTtJrAGjXElg","colab_type":"code","outputId":"4a885a37-91c7-45ec-cc21-dc25d8bcdcbb","executionInfo":{"status":"ok","timestamp":1588112411896,"user_tz":300,"elapsed":11948,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["print(net)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["BaseNet(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n","  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n","  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n","  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc_net): Sequential(\n","    (0): Linear(in_features=256, out_features=128, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Linear(in_features=128, out_features=5, bias=True)\n","    (3): ReLU(inplace=True)\n","    (4): Linear(in_features=5, out_features=10, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zAZjIcLOdp-W","colab_type":"code","colab":{}},"source":["########################################################################\n","# 3. Define a Loss function and optimizer\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","# Here we use Cross-Entropy loss and SGD with momentum.\n","# The CrossEntropyLoss criterion already includes softmax within its\n","# implementation. That's why we don't use a softmax in our model\n","# definition.\n","\n","import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","\n","# Tune the learning rate.\n","# See whether the momentum is useful or not\n","optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n","\n","plt.ioff()\n","fig = plt.figure()\n","train_loss_over_epochs = []\n","val_accuracy_over_epochs = []\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ku7eF366dyUP","colab_type":"code","outputId":"586465d5-d4c8-499e-fed9-d80e70af2c1f","executionInfo":{"status":"ok","timestamp":1588112631356,"user_tz":300,"elapsed":231390,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":593,"referenced_widgets":["b71add10229c4282a57002ae17efe97b","ab31654277c243bcb8dc1bbf1587d6a3","b44155713f0c4b4c932e7851a5e92b86","e1a051c2d8bf4e0b83bc52722b1a261e","72ce018a7f8f405eb50667a7bb728f93","b9bcca17d99f4964aff257ff29c7f478","1b207a6636af4103b0a7de4b5298371e","f755eaac934e48d98f8e255706fbbfd0"]}},"source":["########################################################################\n","# 4. Train the network\n","# ^^^^^^^^^^^^^^^^^^^^\n","#\n","# We simply have to loop over our data iterator, and feed the inputs to the\n","# network and optimize. We evaluate the validation accuracy at each\n","# epoch and plot these values over the number of epochs\n","# Nothing to change here\n","# -----------------------------\n","for epoch in tqdm(range(EPOCHS), total=EPOCHS):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","\n","        if IS_GPU:\n","            inputs = inputs.cuda()\n","            labels = labels.cuda()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","    \n","    # Normalizing the loss by the total number of train batches\n","    running_loss/=len(trainloader)\n","    print('[%d] loss: %.3f' %\n","          (epoch + 1, running_loss))\n","\n","    # Scale of 0.0 to 100.0\n","    # Calculate validation set accuracy of the existing model\n","    val_accuracy, val_classwise_accuracy = \\\n","        calculate_accuracy(valloader, IS_GPU)\n","    print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n","\n","    # # Optionally print classwise accuracies\n","    # for c_i in range(TOTAL_CLASSES):\n","    #     print('Accuracy of %5s : %2d %%' % (\n","    #         classes[c_i], 100 * val_classwise_accuracy[c_i]))\n","\n","    train_loss_over_epochs.append(running_loss)\n","    val_accuracy_over_epochs.append(val_accuracy)\n","# -----------------------------\n","\n","\n","# Plot train loss over epochs and val set accuracy over epochs\n","# Nothing to change here\n","# -------------\n","plt.subplot(2, 1, 1)\n","plt.ylabel('Train loss')\n","plt.plot(np.arange(EPOCHS), train_loss_over_epochs, 'k-')\n","plt.title('train loss and val accuracy')\n","plt.xticks(np.arange(EPOCHS, dtype=int))\n","plt.grid(True)\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(np.arange(EPOCHS), val_accuracy_over_epochs, 'b-')\n","plt.ylabel('Val accuracy')\n","plt.xlabel('Epochs')\n","plt.xticks(np.arange(EPOCHS, dtype=int))\n","plt.grid(True)\n","plt.savefig(\"plot.png\")\n","plt.close(fig)\n","print('Finished Training')\n","# -------------"],"execution_count":31,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b71add10229c4282a57002ae17efe97b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[1] loss: 0.486\n","Accuracy of the network on the val images: 87 %\n","[2] loss: 0.294\n","Accuracy of the network on the val images: 89 %\n","[3] loss: 0.242\n","Accuracy of the network on the val images: 91 %\n","[4] loss: 0.208\n","Accuracy of the network on the val images: 90 %\n","[5] loss: 0.181\n","Accuracy of the network on the val images: 91 %\n","[6] loss: 0.157\n","Accuracy of the network on the val images: 91 %\n","[7] loss: 0.137\n","Accuracy of the network on the val images: 91 %\n","[8] loss: 0.119\n","Accuracy of the network on the val images: 91 %\n","[9] loss: 0.106\n","Accuracy of the network on the val images: 91 %\n","[10] loss: 0.091\n","Accuracy of the network on the val images: 91 %\n","[11] loss: 0.079\n","Accuracy of the network on the val images: 92 %\n","[12] loss: 0.068\n","Accuracy of the network on the val images: 91 %\n","[13] loss: 0.060\n","Accuracy of the network on the val images: 91 %\n","[14] loss: 0.053\n","Accuracy of the network on the val images: 91 %\n","[15] loss: 0.048\n","Accuracy of the network on the val images: 92 %\n","\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v1GE8t3mRdy9","colab_type":"code","outputId":"9052ac4f-b074-4e2b-ca4c-a1f94e7912d9","executionInfo":{"status":"ok","timestamp":1588112631887,"user_tz":300,"elapsed":231895,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["########################################################################\n","# 5. Try the network on test data, and report your performance in the submission report\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","########################################################################\n","\n","# Check out why .eval() is important!\n","# https://discuss.pytorch.org/t/model-train-and-model-eval-vs-model-and-model-eval/5744/2\n","net.eval()\n","\n","test_accuracy, test_classwise_accuracy = \\\n","        calculate_accuracy(testloader, IS_GPU)\n","print('Accuracy of the network on the test images: %d %%' % (test_accuracy))\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Accuracy of the network on the test images: 92 %\n"],"name":"stdout"}]}]}